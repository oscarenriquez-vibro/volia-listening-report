{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c02cb46-43cc-471d-99ca-de8716e7b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Image, display, Audio\n",
    "import ipywidgets as widgets\n",
    "\n",
    "with open(\"custom.css\") as f:\n",
    "    styles = f.read()\n",
    "\n",
    "display(HTML(f\"<style>{styles}</style>\"))\n",
    "\n",
    "# Create password widget\n",
    "password_widget = widgets.Password(\n",
    "    description='Password:',\n",
    "    placeholder='Enter password',\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "assert password_widget.value == \"ommatidia2025\", \"Access denied.\"\n",
    "\n",
    "# Function to validate password\n",
    "def check_password(change):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        if password_widget.value == \"ommatidia2025\":\n",
    "            display(HTML(\"<b style='color:green;'>‚úÖ Access granted. Loading report...</b>\"))\n",
    "        else:\n",
    "            display(HTML(\"<b style='color:red;'>‚ùå Incorrect password. Try again.</b>\"))\n",
    "\n",
    "password_widget.observe(check_password, names='value')\n",
    "\n",
    "# Display the widgets\n",
    "display(HTML(\"<h3>üîí This report is password protected</h3>\"))\n",
    "display(password_widget, output)\n",
    "\n",
    "\n",
    "\n",
    "# Global counter for automatic figure numbering\n",
    "_figure_counter = {\"count\": 0}\n",
    "\n",
    "def captioned_image(filename, caption=\"\", width=400, auto_number=True):\n",
    "    if auto_number:\n",
    "        _figure_counter[\"count\"] += 1\n",
    "        number = _figure_counter[\"count\"]\n",
    "        full_caption = f\"Figure {number}. {caption}\"\n",
    "    else:\n",
    "        full_caption = caption\n",
    "\n",
    "    html = f'''\n",
    "    <div style=\"text-align: center;\">\n",
    "        <img src=\"{filename}\" width=\"{width}\">\n",
    "        <div class=\"caption\">{full_caption}</div>\n",
    "    </div>\n",
    "    '''\n",
    "    display(HTML(html))\n",
    "\n",
    "def captioned_ipy_audio(filename, caption=\"\", auto_number=False):\n",
    "    if auto_number:\n",
    "        _figure_counter[\"count\"] += 1\n",
    "        number = _figure_counter[\"count\"]\n",
    "        full_caption = f\"Audio {number}: {caption}\"\n",
    "    else:\n",
    "        full_caption = caption\n",
    "\n",
    "    display(Audio(filename))\n",
    "    display(HTML(f'<div class=\"caption\">{full_caption}</div>'))\n",
    "\n",
    "def get_transcription(audio_file, cache_file):\n",
    "    \"\"\"\n",
    "    Transcribes audio using Whisper or loads from .txt cache if available.\n",
    "\n",
    "    Parameters:\n",
    "        audio_file (str): Path to the audio file (.wav)\n",
    "        cache_file (str): Path to the .txt file to cache the transcription\n",
    "\n",
    "    Returns:\n",
    "        str: The transcribed text\n",
    "    \"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            transcription = f.read()\n",
    "    else:\n",
    "        transcription = transcribe_audio(audio_file)\n",
    "        with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(transcription)\n",
    "    return transcription\n",
    "\n",
    "def collapsible_transcript_from_file(txt_path, title=\"Click to show transcription\"):\n",
    "    \"\"\"\n",
    "    Loads a transcript from a text file and displays it in a collapsible widget.\n",
    "    \n",
    "    Parameters:\n",
    "        txt_path (str): Path to the .txt file\n",
    "        title (str): Accordion title label\n",
    "    \"\"\"\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        transcript_text = f.read()\n",
    "    \n",
    "    accordion = widgets.Accordion(children=[widgets.HTML(value=f\"<div class='transcript'>{transcript_text}</div>\")])\n",
    "    accordion.set_title(0, title)\n",
    "    display(accordion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f059c-92ed-4eaf-83ca-300f4b642b9f",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "  <img src=\"ommatidia_logo_trbk.png\" width=\"250\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90681a49-65bb-449a-a350-2d8439047d6e",
   "metadata": {},
   "source": [
    "\n",
    "# Listening tests with Q-Series massively parallel infrared laser vibrometers\n",
    "\n",
    "## Objective\n",
    "Evaluate whether Ommatidia's Q-Series systems can reconstruct sound signals by measuring the vibrations of objects under the action of sound waves.\n",
    "\n",
    "## Method\n",
    "We used a pair of simple and small computer speakers, placed behind a piece of paper, while playing music or speech at low volume (only audible for someone nearby the setup). The paper was loosely held on with tape on two of its edges onto an aluminium frame placed atop a table (see image). The table was not very sturdy, and the computer for operating the setup was also on it. Whenever someone walked nearby or when using the mouse or keyboard, the piece of paper would visibly shake in response. We decided not to try to counter any of these random noise sources as a way to evaluate the system's resilience.\n",
    "\n",
    "We took short measurements (15-20 s) applying a real-time Phase-Locked Loop (PLL) filter with a Loop frequency of 1000 Hz and saving the resulting velocity signals for each channel. Afterwards, we applied a simple filtering to the speech test, consisting of a lowpass filter with 8000 Hz cutoff frequency to eliminate high-frequency noise and a bandpass filter from 300-3400 Hz to enhance the typical frequencies of human voice. Finally, we used openai's whisper tool to transcribe the filtered signal files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94b5537-43ea-4bc6-8274-741934f5347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "captioned_image(\"listening_setup.png\", \n",
    "                \"Test setup: A pair of small speakers behind a piece of paper. View from the RGB camara of the Q2 unit. Green dots indicate the approximate position of its 65 infrared laser beams (channels).\",\n",
    "               width=400)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94cee5-8281-4255-9dc6-344499f97e57",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The oscilloscope function of the Q-series systems allows a real time visualization of the velocity and raw signals. We used this to verify that the raw signal intensity is appropriate. The amplitude of the raw signal in these measurements is excellent. Furthermore, we can see that the velocity is indeed small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc8bc94-4835-40c9-82a7-4a0589d328bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "captioned_image(\"trump_oscilloscope.png\", \n",
    "                \"Oscilloscope view of channel 40, which is aimed at the paper, during speech measurement. The rms value of the velocity signal along the 20 s of the measurement is approximately 0.19 mm/s\",\n",
    "               width=500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367bc681-b9b0-41c7-b257-8d2eadcb5cf6",
   "metadata": {},
   "source": [
    "### First test: music\n",
    "The following audio corresponds to the signal recorder by the Q2 without any further filtering. Play it and see if you can identify the song (using headphones yields a clearer audio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db418474-e45f-4e88-814e-dcef8790c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "captioned_ipy_audio(\"starwars_ch4_unfiltered.wav\", \"Audio test 1: Music.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536111a4-260c-48c4-9c79-fe198d2decec",
   "metadata": {},
   "source": [
    "### Second test: speech\n",
    "The following audio corresponds to the signal recorder by the Q2 with the lowpass and bandpass filtering described above. We found that whisper did a better job transcribing with this additional filtering.\n",
    "\n",
    "Play it and see if you can identify the speaker and the topic (using headphones yields a clearer audio). After listening to it, you can view whisper's transcription by clicking on the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4998ea-aad8-4591-8934-9694656eded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "captioned_ipy_audio(\"Potus_ch25_filtered.wav\", \"Audio test 2: Speech.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a96c61-2fc6-4047-b748-d9641eba198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsible_transcript_from_file(\"Potus_filtered_transcription.txt\")\n",
    "display(HTML('<div style=\"margin-bottom: 40px;\"></div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e342fe2-5d94-4a86-9af5-98163d6d587a",
   "metadata": {},
   "source": [
    "## Conclusion and recommendations for further steps\n",
    "\n",
    "This was a successful proof of concept towards using our techology as an infrared listening device.\n",
    "The desired application does not require fine-resolution scanning capabilities. Hence, our Q1S device appears as the ideal choice. Its laser power amplifier makes it especially suited for measurements from long distance, at small incidence angles, or with very small vibration amplitude.\n",
    "\n",
    "Fine-tuning the Q1S as a listening device could entail developing software features than can automatically identify channels with the best signal intensity and focus on those to provide almost-real-time listening capabilty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a63b9-ccf6-472d-83c1-5171683dd695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today().isoformat()\n",
    "\n",
    "display(HTML(f'''\n",
    "    <div style=\"text-align: right; font-size: 11px; color: #777; margin-top: 60px;\">\n",
    "        Measurements, processing and report by Oscar Enr√≠quez, Product Specialist at Ommatidia<br>\n",
    "        Last update {today}\n",
    "    </div>\n",
    "'''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
